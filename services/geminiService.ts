
import { GoogleGenAI } from "@google/genai";
import { ChatMessage } from "../types";

/**
 * Servi√ßo otimizado para a Web R√°dio Figueir√≥.
 * Focado em streaming imediato e baix√≠ssima lat√™ncia.
 */
export const getRadioAssistantStream = async (
  history: ChatMessage[], 
  message: string, 
  onChunk: (text: string) => void
) => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    
    const agora = new Date();
    const timeStr = agora.toLocaleTimeString('pt-PT', { hour: '2-digit', minute: '2-digit' });
    
    // Limpeza agressiva de hist√≥rico para evitar erros de contexto
    // Filtramos erros anteriores e limitamos a 2 mensagens (1 par user/model)
    const cleanHistory = history
      .filter(msg => 
        msg.text.length > 0 && 
        !msg.text.includes("interfer√™ncia") && 
        !msg.text.includes("Ups!")
      )
      .slice(-2);

    const contents: any[] = [];
    cleanHistory.forEach((msg) => {
      const role = msg.role === 'user' ? 'user' : 'model';
      // Garante altern√¢ncia estrita user -> model -> user
      if (contents.length === 0 || contents[contents.length - 1].role !== role) {
        contents.push({ role, parts: [{ text: msg.text }] });
      }
    });

    const result = await ai.models.generateContentStream({
      model: 'gemini-3-flash-preview',
      contents: [
        ...contents,
        { role: 'user', parts: [{ text: message }] }
      ],
      config: {
        thinkingConfig: { thinkingBudget: 0 }, // Resposta imediata
        systemInstruction: `√âs o "Figueir√≥ AI", o locutor digital da Web R√°dio Figueir√≥. üìª
        Contexto: Direto de Figueir√≥, Portugal. Hora: ${timeStr}.
        
        ESTILO:
        - Curto, en√©rgico e carism√°tico.
        - M√°ximo de 15 palavras.
        - Usa g√≠ria de r√°dio: "estamos juntos", "na melhor companhia", "forte abra√ßo".
        
        REGRAS:
        - Se pedirem m√∫sica: sugere Ivandro, Nininho Vaz Maia ou Tony Carreira.
        - Se falarem de viagens/carros: elogia a "FM Rent a Car".
        - Responde como se tivesses o microfone aberto agora mesmo!`,
        temperature: 0.9,
        maxOutputTokens: 80,
      },
    });

    let accumulatedText = "";
    for await (const chunk of result.stream) {
      const chunkText = chunk.text;
      if (chunkText) {
        accumulatedText += chunkText;
        onChunk(accumulatedText);
      }
    }

    return accumulatedText;

  } catch (error) {
    console.error("Erro no Fluxo Gemini:", error);
    throw error;
  }
};
